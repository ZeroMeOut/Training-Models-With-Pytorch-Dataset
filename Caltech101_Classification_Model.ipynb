{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZM2nD-p6WaS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets \n",
        "import torch.utils.data as data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "rQTxpX3Y7WYp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caltech = torchvision.datasets.Caltech101(\"/content/\", transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU_DGMKs7XCw",
        "outputId": "e38c0749-a6e3-4471-9491-f56b0b832d5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torchvision.datasets.ImageFolder(root='/content/caltech101/101_ObjectCategories/', transform=transform)"
      ],
      "metadata": {
        "id": "2ls4fQXuBHw6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "FpPBp3ulCCP7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Wa7skg-c3Z",
        "outputId": "74d669f2-bdb4-4126-a2c0-49f8bdcf3690"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,param in enumerate(model.parameters()):\n",
        "    if i<100:\n",
        "        param.requires_grad=False"
      ],
      "metadata": {
        "id": "62nnhjVnDJuU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "epochs = 8\n",
        "model_name = '/content/drive/MyDrive/Caltech101-Classification-Model.pt'\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=4e-5,weight_decay=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=1,verbose=True)\n",
        "\n",
        "writer = SummaryWriter() # For Tensorboard\n",
        "batch_size = 32\n",
        "\n",
        "early_stop_count=0\n",
        "ES_patience=5\n",
        "best = 0.0\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    train_loss = 0.0\n",
        "    tbar = tqdm(train_loader, desc = 'Training', position=0, leave=True)\n",
        "    for i,(inp,lbl) in enumerate(tbar):\n",
        "        optimizer.zero_grad()\n",
        "        if cuda:\n",
        "            inp,lbl = inp.cuda(),lbl.cuda()\n",
        "        out = model(inp)\n",
        "        loss = criterion(out,lbl)\n",
        "        train_loss += loss\n",
        "        out = out.argmax(dim=1)\n",
        "        correct += (out == lbl).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        tbar.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.5f}, acc: {100.0*correct/((i+1)*train_loader.batch_size):.4f}%\")\n",
        "    train_acc = 100.0*correct/len(train_loader.dataset)\n",
        "    train_loss /= (len(train_loader.dataset)/batch_size)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        val_loss = 0.0\n",
        "        vbar = tqdm(val_loader, desc = 'Validation', position=0, leave=True)\n",
        "        for i,(inp,lbl) in enumerate(vbar):\n",
        "            if cuda:\n",
        "                inp,lbl = inp.cuda(),lbl.cuda()\n",
        "            out = model(inp)\n",
        "            val_loss += criterion(out,lbl)\n",
        "            out = out.argmax(dim=1)\n",
        "            correct += (out == lbl).sum().item()\n",
        "        val_acc = 100.0*correct/len(val_loader.dataset)\n",
        "        val_loss /= (len(val_loader.dataset)/batch_size)\n",
        "    print(f'\\nEpoch: {epoch+1}/{epochs}')\n",
        "    print(f'Train loss: {train_loss}, Train Accuracy: {train_acc}')\n",
        "    print(f'Validation loss: {val_loss}, Validation Accuracy: {val_acc}\\n')\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # write to tensorboard\n",
        "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
        "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "\n",
        "    if val_acc>best:\n",
        "        best=val_acc\n",
        "        torch.save(model,model_name)\n",
        "        early_stop_count=0\n",
        "        print('Accuracy Improved, model saved.\\n')\n",
        "    else:\n",
        "        early_stop_count+=1\n",
        "\n",
        "    if early_stop_count==ES_patience:\n",
        "        print('Early Stopping Initiated...')\n",
        "        print(f'Best Accuracy achieved: {best:.2f}% at epoch:{epoch-ES_patience}')\n",
        "        print(f'Model saved as {model_name}')\n",
        "        break\n",
        "    writer.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZrouY_XDTGL",
        "outputId": "1335993f-e6dc-4e1e-8249-730dd80d2fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 1, loss: 0.87868, acc: 95.0191%: 100%|██████████| 229/229 [02:52<00:00,  1.32it/s]\n",
            "Validation: 100%|██████████| 58/58 [00:20<00:00,  2.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/8\n",
            "Train loss: 0.20430290699005127, Train Accuracy: 95.18796992481202\n",
            "Validation loss: 0.3855472505092621, Validation Accuracy: 90.75997813012575\n",
            "\n",
            "Accuracy Improved, model saved.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 2, loss: 0.01695, acc: 98.6217%: 100%|██████████| 229/229 [02:51<00:00,  1.33it/s]\n",
            "Validation: 100%|██████████| 58/58 [00:20<00:00,  2.81it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/8\n",
            "Train loss: 0.05237208306789398, Train Accuracy: 98.796992481203\n",
            "Validation loss: 0.35502588748931885, Validation Accuracy: 92.72826681246583\n",
            "\n",
            "Accuracy Improved, model saved.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 3, loss: 0.02892, acc: 99.3859%: 100%|██████████| 229/229 [02:53<00:00,  1.32it/s]\n",
            "Validation: 100%|██████████| 58/58 [00:20<00:00,  2.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/8\n",
            "Train loss: 0.02615213952958584, Train Accuracy: 99.56254272043746\n",
            "Validation loss: 0.32828736305236816, Validation Accuracy: 93.60306178239475\n",
            "\n",
            "Accuracy Improved, model saved.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4, loss: 0.02206, acc: 99.6632%:  73%|███████▎  | 167/229 [02:07<00:46,  1.33it/s]"
          ]
        }
      ]
    }
  ]
}